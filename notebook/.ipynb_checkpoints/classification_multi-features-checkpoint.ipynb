{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d721ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne \n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy import stats\n",
    "from matplotlib.backends.backend_pdf import PdfPages \n",
    "\n",
    "from src.params import SUBJ_CLEAN, RESULT_PATH, ACTIVE_RUN, PASSIVE_RUN, FIG_PATH, FREQS_LIST, PREPROC_PATH, FREQS_NAMES\n",
    "from src.utils import get_bids_file\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, RandomizedSearchCV, GridSearchCV, LeaveOneGroupOut, LeaveOneOut\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "from mne.decoding import (\n",
    "    SlidingEstimator,\n",
    "    GeneralizingEstimator,\n",
    "    Scaler,\n",
    "    cross_val_multiscore,\n",
    "    LinearModel,\n",
    "    get_coef,\n",
    "    Vectorizer,\n",
    "    CSP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd17d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_window_hilbert_one_su(subj_list, RUN_LIST, tmin, tmax, cond1, cond2, FREQS_NAMES, FREQS_LIST, save=True) :\n",
    "    \n",
    "    print('Time window : ' + str(tmin) + '-' + str(tmax))\n",
    "\n",
    "    FREQS = [x for x in range(len(FREQS_LIST))]\n",
    "    idx = 0 #TODO : Change that\n",
    "    \n",
    "    for l_freq, h_freq in FREQS_LIST :\n",
    "        print('Processing freqs -->', l_freq, h_freq)\n",
    "        \n",
    "        for i_subj, subj in enumerate(subj_list):\n",
    "            print(\"=> Process task :\", task, 'subject', subj)\n",
    "            \n",
    "            epochs_list = []\n",
    "\n",
    "            y_run_cond1 = []\n",
    "            y_run_cond2 = []\n",
    "\n",
    "            X_subj = []\n",
    "            y_subj = []\n",
    "\n",
    "            X_cond1 = []\n",
    "            X_cond2 = []\n",
    "\n",
    "            power_list = []\n",
    "            \n",
    "            group = []\n",
    "\n",
    "            sub_id = 'sub-' + subj\n",
    "            subj_path = os.path.join(RESULT_PATH, 'meg', 'reports', sub_id)\n",
    "\n",
    "            if not os.path.isdir(subj_path):\n",
    "                os.mkdir(subj_path)\n",
    "                print('BEHAV folder created at : {}'.format(subj_path))\n",
    "            else:\n",
    "                print('{} already exists.'.format(subj_path))\n",
    "\n",
    "            # Take ica data\n",
    "            _, ica_path = get_bids_file(PREPROC_PATH, stage='ica', subj=subj)\n",
    "            ica = mne.preprocessing.read_ica(ica_path)\n",
    "        \n",
    "            # Select rejection threshold associated with each run\n",
    "            # TODO : apply threshold per run\n",
    "            print('Load reject threshold')\n",
    "            _, reject_path = get_bids_file(RESULT_PATH, stage=\"AR_epo\", task=task, subj=subj, measure='log')\n",
    "\n",
    "            with open(reject_path, 'rb') as f:\n",
    "                reject = pickle.load(f)  \n",
    "\n",
    "            # Select event_id appropriate\n",
    "            for run in RUN_LIST :\n",
    "                print(\"=> Process run :\", run)\n",
    "                        \n",
    "                if task == 'LaughterPassive' :\n",
    "                    if run == '02' or run == '03' :\n",
    "                        event_id = {'LaughReal' : 11, 'LaughPosed' : 12, 'EnvReal' : 21, \n",
    "                                    'EnvPosed' : 22}\n",
    "                    elif run == '04' or run == '05' :\n",
    "                        event_id = {'LaughReal' : 11, 'LaughPosed' : 12,'ScraReal' : 31, \n",
    "                                    'ScraPosed' : 32,}\n",
    "                if task == 'LaughterActive' :\n",
    "                        event_id = {'LaughReal' : 11, 'LaughPosed' : 12, 'Good' : 99, 'Miss' : 66}\n",
    "\n",
    "\n",
    "                # Take raw data filtered : i.e. NO ICA \n",
    "                print('ICA')\n",
    "                _, raw_path = get_bids_file(PREPROC_PATH, stage='proc-filt_raw', task=task, run=run, subj=subj)\n",
    "                raw = mne.io.read_raw_fif(raw_path, preload=True)\n",
    "                raw_filter = raw.copy()\n",
    "                raw_filter = ica.apply(raw_filter)\n",
    "\n",
    "                epochs_psds = []\n",
    "\n",
    "                freq_name = FREQS_NAMES[idx]\n",
    "\n",
    "                print('Apply filter')\n",
    "                info = raw_filter.info\n",
    "                raw_filter.filter(l_freq, h_freq) # Apply filter of interest\n",
    "                raw_hilbert = raw_filter.apply_hilbert(envelope=True)\n",
    "\n",
    "                picks = mne.pick_types(raw.info, meg=True, ref_meg=False, eeg=False, eog=False)\n",
    "\n",
    "                power_hilbert = raw_hilbert.copy()\n",
    "                power_hilbert._data = power_hilbert._data**2\n",
    "\n",
    "                # Segmentation\n",
    "                print('Segmentation')\n",
    "                events = mne.find_events(raw)\n",
    "                epochs = mne.Epochs(\n",
    "                    power_hilbert,\n",
    "                    events=events,\n",
    "                    event_id=event_id,\n",
    "                    tmin=tmin,\n",
    "                    tmax=tmax,\n",
    "                    baseline=(tmin, tmin),\n",
    "                    picks=picks)    \n",
    "\n",
    "                # Auto-reject epochs\n",
    "                epochs.drop_bad(reject=reject)\n",
    "\n",
    "                if subj == subj_list[0] and run == run_list[0]:\n",
    "                    head_info = epochs.info['dev_head_t']\n",
    "                else:\n",
    "                    epochs.info['dev_head_t'] = head_info\n",
    "\n",
    "                epochs.equalize_event_counts([cond1, cond2])\n",
    "                \n",
    "                for i_cond, cond in enumerate(epochs[cond1, cond2].get_data()) : \n",
    "                    group.append(i_subj)\n",
    "                    \n",
    "                epochs_list.append(epochs)\n",
    "\n",
    "                del epochs\n",
    "                \n",
    "            epochs_all_run = mne.concatenate_epochs(epochs_list)  \n",
    "\n",
    "            print(epochs_all_run)\n",
    "\n",
    "            # Compute power (= envelop **2)\n",
    "            print('Calculte power')\n",
    "            ave_time_cond1 = np.mean(epochs_all_run[cond1].get_data(), axis=2)\n",
    "            ave_time_cond2 = np.mean(epochs_all_run[cond2].get_data(), axis=2)\n",
    "\n",
    "            X_cond1.append(ave_time_cond1)\n",
    "            X_cond2.append(ave_time_cond2)\n",
    "            print('Cond1 shape :', X_cond1)\n",
    "            print('Cond2 shape :', X_cond2)\n",
    "\n",
    "            print('Concatenation')\n",
    "            X_subj.append(np.concatenate((ave_time_cond1, ave_time_cond2)))\n",
    "            y_subj.append(np.concatenate((epochs_all_run[cond1].events[:, 2], epochs_all_run[cond2].events[:, 2])))\n",
    "\n",
    "            print(X_subj[-1].shape)\n",
    "            print(y_subj[-1].shape)\n",
    "            print(np.array(group).shape)\n",
    "            \n",
    "            # Classification\n",
    "            all_scores = clf_randomSearch(X_subj, y_subj, group)\n",
    "            print(all_scores)\n",
    "            # Plot topomap per subject\n",
    "            fig, ax_topo = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "            vmin = min(all_scores)\n",
    "            vmax = max(all_scores)\n",
    "\n",
    "            img, _ = mne.viz.plot_topomap(all_scores, epochs_all_run.info, axes=ax_topo, show=False, \n",
    "                                    cmap='plasma_r', extrapolate='head',\n",
    "                                    sphere=(0, 0.0, 0, 0.19), vlim = (vmin, vmax))\n",
    "\n",
    "            ax_topo.set_title(freq_name)\n",
    "            \n",
    "            fig.subplots_adjust(right=0.8)\n",
    "            cbar_ax = fig.add_axes([0.85, 0.15, 0.01, 0.7])\n",
    "            fig.colorbar(img, cax=cbar_ax)\n",
    "            cbar_ax.set_label('Accuracy')\n",
    "            \n",
    "            plt.show()\n",
    "    \n",
    "\n",
    "            if save == True : \n",
    "                sub_name = subj\n",
    "                run_name = 'all'\n",
    "                tmin_name = str(int(tmin*1000))\n",
    "                tmax_name = str(int(tmax*1000))\n",
    "                stage = 'ml-'+ tmin_name + '-' + tmax_name\n",
    "                conditions = cond1 + '-' + cond2\n",
    "\n",
    "                save_X = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                    run_name, conditions,\n",
    "                                                                                                    'X-subj', stage)\n",
    "                save_y = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                    run_name, conditions,'y-subj', stage)\n",
    "\n",
    "                save_group = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                    run_name, conditions,\n",
    "                                                                                                    'group', stage)\n",
    "\n",
    "                save_X1 = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                    run_name, conditions,\n",
    "                                                                                                    'X-cond1', stage)\n",
    "\n",
    "                save_X2 = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                    run_name, conditions,\n",
    "                                                                                                    'X-cond2', stage)\n",
    "\n",
    "                with open(save_X, 'wb') as f:\n",
    "                    pickle.dump(X_subj, f)\n",
    "\n",
    "                with open(save_y, 'wb') as f:\n",
    "                    pickle.dump(y_subj, f)\n",
    "\n",
    "                with open(save_group, 'wb') as f:\n",
    "                    pickle.dump(group, f)\n",
    "\n",
    "                with open(save_X1, 'wb') as f:\n",
    "                    pickle.dump(X_cond1, f)\n",
    "\n",
    "                with open(save_X2, 'wb') as f:\n",
    "                    pickle.dump(X_cond2, f)\n",
    "\n",
    "    return X_subj, y_subj, group, epochs_all_run, X_cond1, X_cond2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d81b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(tmin, tmax, cond1, cond2, freq_name) : \n",
    "    sub_name = 'all'\n",
    "    run_name = 'all'\n",
    "    tmin_name = str(int(tmin*1000))\n",
    "    tmax_name = str(int(tmax*1000))\n",
    "    stage = 'ml-'+ tmin_name + '-' + tmax_name\n",
    "    conditions = cond1 + '-' + cond2\n",
    "\n",
    "    save_X = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-sf-{}_freq-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                        run_name, conditions,\n",
    "                                                                                                        'X-subj', freq_name, stage)\n",
    "    \n",
    "    save_y = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-sf-{}_freq-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                        run_name, conditions,\n",
    "                                                                                                        'y-subj', freq_name, stage)\n",
    "\n",
    "    save_group = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-sf-{}_freq-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                            run_name, conditions,\n",
    "                                                                                                            'group', freq_name, stage)\n",
    "\n",
    "    save_X1 = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-sf-{}_freq-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                        run_name, conditions,\n",
    "                                                                                                        'X-cond1', freq_name, stage)\n",
    "\n",
    "    save_X2 = RESULT_PATH + 'meg/reports/sub-all/ml/sub-{}_task-{}_run-{}_cond-{}_meas-sf-{}_freq-{}_{}.pkl'.format(sub_name, task, \n",
    "                                                                                                        run_name, conditions,\n",
    "                                                                                                        'X-cond2', freq_name, stage)\n",
    "\n",
    "    with open(save_X, 'rb') as f:\n",
    "        X_subj = pickle.load(f)\n",
    "        \n",
    "    with open(save_y, 'rb') as f:\n",
    "        y_subj = pickle.load(f)\n",
    "        \n",
    "    with open(save_group, 'rb') as f:\n",
    "        group = pickle.load(f)\n",
    "        \n",
    "    with open(save_X1, 'rb') as f:\n",
    "        X_cond1 = pickle.load(f)\n",
    "\n",
    "    with open(save_X2, 'rb') as f:\n",
    "        X_cond2 = pickle.load(f)\n",
    "        \n",
    "    return X_subj, y_subj, group, X_cond1, X_cond2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734295bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_randomSearch(X_subj, y_subj, group) : \n",
    "    all_scores = []\n",
    "\n",
    "    # Select the classifier & cross-val method\n",
    "    X = X_subj[-1] #X_subj[-1]\n",
    "    print(X.shape)\n",
    "    X_classif = X\n",
    "    print(X_classif.shape)\n",
    "    \n",
    "\n",
    "    y = y_subj[-1]\n",
    "    print(y.shape)\n",
    "    \n",
    "    groups = np.array(group)\n",
    "    print(groups.shape)\n",
    "\n",
    "    # TODO : Separate in X_train and y_train, X_test, y_test\n",
    "    y_pred = []\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [2, 5, 10, 15]\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [10, 20, 25]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the param grid\n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'bootstrap': bootstrap}\n",
    "    print(param_grid)\n",
    "    \n",
    "    cv = LeaveOneGroupOut() # Cross-validation via LOGO\n",
    "\n",
    "    # Select channel of interest\n",
    "    #X_chan = X[:, chan]\n",
    "    #X_classif = X_chan.reshape(-1, 1) # Reshape (n_event, 1)\n",
    "\n",
    "    # Find best params with GridSearch\n",
    "    # Use RandomSearch\n",
    "    print('---->Find best params')\n",
    "    search = RandomizedSearchCV(clf, param_grid, cv=cv, verbose=True, n_jobs=-1).fit(X=X_classif, \n",
    "                                                                                    y=y, \n",
    "                                                                                    groups=groups)\n",
    "\n",
    "    # Print scores for each fold\n",
    "    print('Scores for each fold:')\n",
    "    for i, score in enumerate(search.cv_results_['split0_test_score']):\n",
    "        print(f'Fold {i + 1}: {score}')\n",
    "\n",
    "    # Select best params\n",
    "    best_params = search.best_estimator_\n",
    "    print('Best params : ' + str(best_params))\n",
    "\n",
    "    # Accuracy score\n",
    "    scores = search.score(X=X_classif, y=y) \n",
    "    print('Scores', scores)\n",
    "    print('Best scores', search.best_score_)\n",
    "    \n",
    "    feature_importance = search.best_estimator_.feature_importances_\n",
    "    print('Feature Importance:', feature_importance)    \n",
    "\n",
    "    all_scores.append(scores)\n",
    "    print(all_scores)\n",
    "    \n",
    "    return all_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8174a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240, 270)\n",
      "(3240, 270)\n",
      "(3240,)\n",
      "(3240,)\n",
      "{'n_estimators': [10, 17, 25, 33, 41, 48, 56, 64, 72, 80], 'max_features': ['sqrt'], 'max_depth': [2, 5, 10, 15], 'min_samples_split': [10, 20, 25], 'min_samples_leaf': [1, 2], 'bootstrap': [True, False]}\n",
      "---->Find best params\n",
      "Fitting 27 folds for each of 10 candidates, totalling 270 fits\n",
      "Scores for each fold:\n",
      "Fold 1: 0.0\n",
      "Fold 2: 0.0\n",
      "Fold 3: 0.0\n",
      "Fold 4: 0.0\n",
      "Fold 5: 0.0\n",
      "Fold 6: 0.0\n",
      "Fold 7: 0.0\n",
      "Fold 8: 0.0\n",
      "Fold 9: 0.0\n",
      "Fold 10: 0.0\n",
      "Best params : RandomForestClassifier(max_depth=15, min_samples_split=10, n_estimators=80)\n",
      "Scores 0.5\n",
      "Best scores 0.018518518518518517\n",
      "Feature Importance: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "######## MAIN\n",
    "task = 'LaughterActive'\n",
    "subj_list = SUBJ_CLEAN\n",
    "\n",
    "if task == 'LaughterActive':\n",
    "    run_list = ACTIVE_RUN\n",
    "elif task == 'LaughterPassive':\n",
    "    run_list = PASSIVE_RUN\n",
    "\n",
    "# Select what conditions to compute (str)\n",
    "cond1 = 'LaughReal'\n",
    "cond2 = 'LaughPosed'\n",
    "freq_name = 'beta'\n",
    "conditions = cond1 + '-' + cond2\n",
    "condition_list = [cond1, cond2]\n",
    "picks = \"meg\" # Select MEG channels\n",
    "time_window = False\n",
    "\n",
    "tmin = 0.0\n",
    "tmax = 0.2\n",
    "\n",
    "frequency_list = []\n",
    "for i, freq in enumerate(FREQS_NAMES) : \n",
    "    if freq == freq_name :\n",
    "        frequency_list.append(FREQS_LIST[i])\n",
    "        \n",
    "X_subj, y_subj, group, X_cond1, X_cond2 = upload_data(tmin, tmax, cond1, cond2, freq_name)\n",
    "#X_subj = np.random.rand(3240, 270)\n",
    "all_scores = clf_randomSearch(X_subj, y_subj, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0c21167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240, 3)\n"
     ]
    }
   ],
   "source": [
    "X = X_subj[-1]\n",
    "X = X[:, :3]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## PLOT TOPOMAPS ########\n",
    "for subj in ['all'] :\n",
    "    sub_name = 'all'\n",
    "    run_name = 'all'\n",
    "    tmin_name = str(int(tmin*1000))\n",
    "    tmax_name = str(int(tmax*1000))\n",
    "    stage = tmin_name + '-' + tmax_name\n",
    "    conditions = cond1 + '-' + cond2\n",
    "\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "\n",
    "    # initialize figure\n",
    "    fig, ax_topo = plt.subplots(1, len(FREQS_NAMES), figsize=(30, 6))\n",
    "\n",
    "    for i, freq_name in enumerate(FREQS_NAMES): \n",
    "\n",
    "        # Load data\n",
    "        save_scores = RESULT_PATH + 'meg/reports/sub-{}/ml/results_single_feat/sub-{}_task-{}_run-{}_cond-{}_meas-sf-{}_freq_{}{}.pkl'.format(sub_name, sub_name, task, \n",
    "                                                                                                                                    run_name, conditions,\n",
    "                                                                                                                                    'scores-svm', freq_name, stage)\n",
    "        with open(save_scores, 'rb') as f:\n",
    "            all_scores = pickle.load(f)\n",
    "\n",
    "        minimum = np.min(all_scores)\n",
    "        min_list.append(minimum)\n",
    "\n",
    "        maximum = np.max(all_scores)\n",
    "        max_list.append(maximum)\n",
    "\n",
    "\n",
    "\n",
    "    for i, freq_name in enumerate(FREQS_NAMES): \n",
    "\n",
    "        # Load data\n",
    "        save_scores = RESULT_PATH + 'meg/reports/sub-{}/ml/results_single_feat/sub-{}_task-{}_run-{}_cond-{}_meas-sf-{}_freq_{}{}.pkl'.format(sub_name, sub_name, task, \n",
    "                                                                                                                                    run_name, conditions,\n",
    "                                                                                                                                    'scores-svm', freq_name, stage)\n",
    "        with open(save_scores, 'rb') as f:\n",
    "            all_scores = pickle.load(f)\n",
    "\n",
    "        vmin = min(min_list)\n",
    "        vmax = max(max_list)\n",
    "\n",
    "        img, _ = mne.viz.plot_topomap(all_scores, epochs.info, axes=ax_topo[i], show=False, \n",
    "                                cmap='plasma_r', extrapolate='head',\n",
    "                                sphere=(0, 0.0, 0, 0.19), vlim = (0.40, 0.75))\n",
    "\n",
    "        ax_topo[i].set_title(freq_name)    \n",
    "\n",
    "    # TODO : put the colorbar on the right\n",
    "    #cbar = plt.colorbar(\n",
    "    #    ax=ax_topo[i], shrink=0.2, orientation='vertical', mappable=img,)\n",
    "\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.01, 0.7])\n",
    "    fig.colorbar(img, cax=cbar_ax)\n",
    "    cbar_ax.set_label('Accuracy')\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
