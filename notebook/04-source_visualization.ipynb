{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f981dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pickle\n",
    "from src.params import MRI_PATH, SUBJ_CLEAN, RESULT_PATH\n",
    "from src.utils import get_bids_file\n",
    "from src.config import subjects_dir\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "from mne.stats import spatio_temporal_cluster_1samp_test, summarize_clusters_stc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d02f85",
   "metadata": {},
   "source": [
    "## Test average condition per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d97263c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/claraelk/scratch/laughter_data/results/meg/reports/sub-01/sub-01_task-LaughterActive_run-all_AR_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 7 columns\n",
      "296 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Cond1 73\n",
      "Cond2 74\n",
      "[[5.69848951e-11 5.96709755e-11 6.04139606e-11 ... 5.78296795e-11\n",
      "  5.89044321e-11 5.81591304e-11]\n",
      " [7.04388674e-11 6.81827466e-11 6.51834231e-11 ... 6.05369134e-11\n",
      "  6.38491105e-11 6.63099388e-11]\n",
      " [4.16683818e-11 4.30833737e-11 4.33898840e-11 ... 4.20610316e-11\n",
      "  4.33418436e-11 4.42929466e-11]\n",
      " ...\n",
      " [1.16494224e-10 1.18782809e-10 1.25432887e-10 ... 1.16477158e-10\n",
      "  1.16553588e-10 1.20627702e-10]\n",
      " [1.18722068e-10 1.19620771e-10 1.24822550e-10 ... 1.14858708e-10\n",
      "  1.14436696e-10 1.17713666e-10]\n",
      " [1.15720023e-10 1.14798506e-10 1.17059149e-10 ... 1.08920257e-10\n",
      "  1.08389159e-10 1.10971285e-10]]\n",
      "Reading /home/claraelk/scratch/laughter_data/results/meg/reports/sub-02/sub-02_task-LaughterActive_run-all_AR_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 7 columns\n",
      "295 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Cond1 75\n",
      "Cond2 72\n",
      "[[2.74344483e-11 2.79506120e-11 2.83988788e-11 ... 2.96249684e-11\n",
      "  3.00492427e-11 2.98648667e-11]\n",
      " [3.50118906e-11 3.36927265e-11 3.27374705e-11 ... 3.78093406e-11\n",
      "  3.71152538e-11 3.63225529e-11]\n",
      " [2.85430437e-11 2.82890979e-11 2.75212660e-11 ... 3.15440830e-11\n",
      "  3.14143309e-11 3.15344330e-11]\n",
      " ...\n",
      " [4.01842221e-11 4.18641595e-11 4.31505787e-11 ... 4.44344594e-11\n",
      "  4.34753075e-11 4.34315071e-11]\n",
      " [4.07678880e-11 4.13826994e-11 4.29284132e-11 ... 4.60797910e-11\n",
      "  4.53124258e-11 4.51680306e-11]\n",
      " [4.04013136e-11 3.98633749e-11 4.14272005e-11 ... 4.64679475e-11\n",
      "  4.67189820e-11 4.74260230e-11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"    print(len(all_stc_cond1)) \\n    print(len(all_stc_cond2)) \\n    \\n    print(stc_cond1.data.shape)\\n\\n    ######## AVERAGE SIGNAL PER CONDITION FOR 1 SUBJ #########\\n    for subject_stcs1 in all_stc_cond1 : \\n\\n        # Average the SourceEstimates for the current subject\\n        avg_data_cond1 = np.mean([s.data for s in all_stc_cond1], axis=0)\\n    \\n        # Append to the list for all subjects\\n        all_subject_stcs_cond1.append(avg_data_cond1)\\n\\n    print('All subj cond1 :', len(all_subject_stcs_cond1)) # Should be n_subj\\n\\n    for subject_stcs2 in all_stc_cond2 : \\n\\n        # Average the SourceEstimates for the current subject\\n        avg_data_cond2 = np.mean([s.data for s in all_stc_cond2], axis=0)\\n\\n        # Append to the list for all subjects\\n        all_subject_stcs_cond2.append(avg_data_cond2)\\n\\n    print('All subj cond2 :', len(all_subject_stcs_cond2)) # Should be n_subj\\n    \\n######## AVERAGE ARRAYS ACROSS SUBJECTS #########\\n # Average all subject all epochs CONDITION 1\\ngroup_avg_data_cond1 = np.mean(all_subject_stcs_cond1, axis=0)\\n\\n# Compute Source Estimate with average value - Cond 1\\ngroup_avg_stc_cond1 = mne.SourceEstimate(group_avg_data_cond1, \\n                                         vertices=stc_cond1.vertices, \\n                                         tmin=stc_cond1.tmin, \\n                                         tstep=stc_cond1.tstep, \\n                                         subject=stc_cond1.subject)\\n\\n# Average all subject all epochs CONDITION 2\\ngroup_avg_data_cond2 = np.mean(all_subject_stcs_cond2, axis=0)\\n\\n# Compute Source Estimate with average value - Cond 2\\ngroup_avg_stc_cond2 = mne.SourceEstimate(group_avg_data_cond2, \\n                                         vertices=stc_cond2.vertices, \\n                                         tmin=stc_cond2.tmin, \\n                                         tstep=stc_cond2.tstep, \\n                                         subject=stc_cond2.subject)\\n\\ngroup_all_stc_cond1.append(group_avg_stc_cond1.data)\\ngroup_all_stc_cond2.append(group_avg_stc_cond2.data)\\n\\n\\nprint(np.array(group_all_stc_cond1).shape)\\nprint(np.array(group_all_stc_cond2).shape)\\n\\n# contrast = np.array(group_all_stc_cond1)-np.array(group_all_stc_cond2) \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### PREPARE DATA #######\n",
    "\n",
    "task = 'LaughterActive'\n",
    "cond1_name = 'LaughReal'\n",
    "cond2_name = 'LaughPosed'\n",
    "\n",
    "####### PREPARE DATA #######\n",
    "\n",
    "# Loop through subjects\n",
    "all_subject_stcs_cond1 = []\n",
    "all_subject_stcs_cond2 = []\n",
    "\n",
    "\n",
    "\n",
    "group_all_stc_cond1 = []\n",
    "group_all_stc_cond2 = []\n",
    "\n",
    "for subj in ['01', '02']:\n",
    "    _, path_epochs = get_bids_file(RESULT_PATH, task=task, subj=subj, stage=\"AR_epo\")\n",
    "    epochs = mne.read_epochs(path_epochs, verbose=None)\n",
    "\n",
    "    # Find condition related to epoch\n",
    "    cond1_epo = []\n",
    "    cond2_epo = []\n",
    "    \n",
    "    all_stc_cond1 = []\n",
    "\n",
    "    all_stc_cond2 = []\n",
    "    for i, epo_arr in enumerate(epochs.events): \n",
    "        if epo_arr[2] == 3 :                \n",
    "            cond1_epo.append(i)\n",
    "        elif epo_arr[2] == 2 : \n",
    "            cond2_epo.append(i)\n",
    "\n",
    "    cond1 = [str(nb).zfill(3) for nb in cond1_epo]\n",
    "    cond2 = [str(nb).zfill(3) for nb in cond2_epo]\n",
    "    print('Cond1', len(cond1))\n",
    "    print('Cond2', len(cond2))\n",
    "    \n",
    "    # Find morph data of the same condition\n",
    "    for epo1 in cond1 : # Cond1\n",
    "        stc_path_cond1 = os.path.join(MRI_PATH, f'sub-{subj}', 'morph',\n",
    "                                    f'{epo1}_MNE_morph-stc.h5')\n",
    "\n",
    "        stc_cond1 = mne.read_source_estimate(stc_path_cond1)\n",
    "        all_stc_cond1.append(stc_cond1)\n",
    "\n",
    "    for epo2 in cond2 : # Cond2\n",
    "        stc_path_cond2 = os.path.join(MRI_PATH, f'sub-{subj}', 'morph',\n",
    "                                    f'{epo2}_MNE_morph-stc.h5')\n",
    "\n",
    "        stc_cond2 = mne.read_source_estimate(stc_path_cond2)\n",
    "        all_stc_cond2.append(stc_cond2)\n",
    "\n",
    "    grand_ave_cond1 = np.mean(all_stc_cond1) \n",
    "    grand_ave_cond2 = np.mean(all_stc_cond2) \n",
    "    \n",
    "    print(grand_ave_cond1.data)\n",
    "\n",
    "\"\"\"    print(len(all_stc_cond1)) \n",
    "    print(len(all_stc_cond2)) \n",
    "    \n",
    "    print(stc_cond1.data.shape)\n",
    "\n",
    "    ######## AVERAGE SIGNAL PER CONDITION FOR 1 SUBJ #########\n",
    "    for subject_stcs1 in all_stc_cond1 : \n",
    "\n",
    "        # Average the SourceEstimates for the current subject\n",
    "        avg_data_cond1 = np.mean([s.data for s in all_stc_cond1], axis=0)\n",
    "    \n",
    "        # Append to the list for all subjects\n",
    "        all_subject_stcs_cond1.append(avg_data_cond1)\n",
    "\n",
    "    print('All subj cond1 :', len(all_subject_stcs_cond1)) # Should be n_subj\n",
    "\n",
    "    for subject_stcs2 in all_stc_cond2 : \n",
    "\n",
    "        # Average the SourceEstimates for the current subject\n",
    "        avg_data_cond2 = np.mean([s.data for s in all_stc_cond2], axis=0)\n",
    "\n",
    "        # Append to the list for all subjects\n",
    "        all_subject_stcs_cond2.append(avg_data_cond2)\n",
    "\n",
    "    print('All subj cond2 :', len(all_subject_stcs_cond2)) # Should be n_subj\n",
    "    \n",
    "######## AVERAGE ARRAYS ACROSS SUBJECTS #########\n",
    " # Average all subject all epochs CONDITION 1\n",
    "group_avg_data_cond1 = np.mean(all_subject_stcs_cond1, axis=0)\n",
    "\n",
    "# Compute Source Estimate with average value - Cond 1\n",
    "group_avg_stc_cond1 = mne.SourceEstimate(group_avg_data_cond1, \n",
    "                                         vertices=stc_cond1.vertices, \n",
    "                                         tmin=stc_cond1.tmin, \n",
    "                                         tstep=stc_cond1.tstep, \n",
    "                                         subject=stc_cond1.subject)\n",
    "\n",
    "# Average all subject all epochs CONDITION 2\n",
    "group_avg_data_cond2 = np.mean(all_subject_stcs_cond2, axis=0)\n",
    "\n",
    "# Compute Source Estimate with average value - Cond 2\n",
    "group_avg_stc_cond2 = mne.SourceEstimate(group_avg_data_cond2, \n",
    "                                         vertices=stc_cond2.vertices, \n",
    "                                         tmin=stc_cond2.tmin, \n",
    "                                         tstep=stc_cond2.tstep, \n",
    "                                         subject=stc_cond2.subject)\n",
    "\n",
    "group_all_stc_cond1.append(group_avg_stc_cond1.data)\n",
    "group_all_stc_cond2.append(group_avg_stc_cond2.data)\n",
    "\n",
    "\n",
    "print(np.array(group_all_stc_cond1).shape)\n",
    "print(np.array(group_all_stc_cond2).shape)\n",
    "\n",
    "# contrast = np.array(group_all_stc_cond1)-np.array(group_all_stc_cond2) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef0a2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 20484, 2401)\n",
      "(27, 20484, 2401)\n"
     ]
    }
   ],
   "source": [
    "task = 'LaughterActive'\n",
    "cond1_name = 'LaughReal'\n",
    "cond2_name = 'LaughPosed'\n",
    "\n",
    "group_all_stc_cond1 = []\n",
    "group_all_stc_cond2 = []\n",
    "\n",
    "for subj in SUBJ_CLEAN :\n",
    "    filename_cond1, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-src', condition=cond1_name)\n",
    "    filename_cond2, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-src', condition=cond2_name)\n",
    "\n",
    "    save_cond1 = os.path.join(MRI_PATH, f'sub-{subj}', filename_cond1[:-4])\n",
    "    save_cond2 = os.path.join(MRI_PATH, f'sub-{subj}', filename_cond2[:-4])\n",
    "\n",
    "    stc_cond1 = mne.read_source_estimate(save_cond1)\n",
    "    stc_cond2 = mne.read_source_estimate(save_cond2)\n",
    "\n",
    "    group_all_stc_cond1.append(stc_cond1.data)\n",
    "    group_all_stc_cond2.append(stc_cond2.data)\n",
    "\n",
    "print(np.array(group_all_stc_cond1).shape)\n",
    "print(np.array(group_all_stc_cond2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e384aa0",
   "metadata": {},
   "source": [
    "## Average subject file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadf7efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute subj-01\n",
      "--> Read Source Estimate\n",
      "Compute subj-02\n",
      "--> Read Source Estimate\n",
      "Compute subj-03\n",
      "--> Read Source Estimate\n",
      "Compute subj-04\n",
      "--> Read Source Estimate\n",
      "Compute subj-05\n",
      "--> Read Source Estimate\n",
      "Compute subj-06\n",
      "--> Read Source Estimate\n",
      "Compute subj-07\n",
      "--> Read Source Estimate\n",
      "Compute subj-08\n",
      "--> Read Source Estimate\n",
      "Compute subj-09\n",
      "--> Read Source Estimate\n",
      "Compute subj-10\n",
      "--> Read Source Estimate\n",
      "Compute subj-11\n",
      "--> Read Source Estimate\n",
      "Compute subj-12\n",
      "--> Read Source Estimate\n",
      "Compute subj-13\n",
      "--> Read Source Estimate\n",
      "Compute subj-14\n",
      "--> Read Source Estimate\n",
      "Compute subj-15\n",
      "--> Read Source Estimate\n",
      "Compute subj-16\n",
      "--> Read Source Estimate\n",
      "Compute subj-18\n",
      "--> Read Source Estimate\n",
      "Compute subj-19\n",
      "--> Read Source Estimate\n",
      "Compute subj-20\n",
      "--> Read Source Estimate\n",
      "Compute subj-21\n",
      "--> Read Source Estimate\n",
      "Compute subj-22\n",
      "--> Read Source Estimate\n",
      "Compute subj-23\n",
      "--> Read Source Estimate\n",
      "Compute subj-24\n",
      "--> Read Source Estimate\n",
      "Compute subj-25\n",
      "--> Read Source Estimate\n",
      "Compute subj-28\n",
      "--> Read Source Estimate\n",
      "Compute subj-30\n",
      "--> Read Source Estimate\n",
      "Compute subj-32\n",
      "--> Read Source Estimate\n",
      "Len condition 1 : 27\n",
      "Len condition 2 : 27\n",
      "Average per condition\n",
      "Average --> Done\n",
      "<SourceEstimate | 20484 vertices, tmin : -500.0 (ms), tmax : 1499.9999523162844 (ms), tstep : 0.8333333134651184 (ms), data shape : (20484, 2401), ~187.8 MB>\n",
      "<SourceEstimate | 20484 vertices, tmin : -500.0 (ms), tmax : 1499.9999523162844 (ms), tstep : 0.8333333134651184 (ms), data shape : (20484, 2401), ~187.8 MB>\n",
      "Save file\n",
      "Overwriting existing file.\n",
      "[done]\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "task = 'LaughterActive'\n",
    "cond1_name = 'LaughReal'\n",
    "cond2_name = 'LaughPosed'\n",
    "\n",
    "all_subj_stc_cond1 = []\n",
    "all_subj_stc_cond2 = []\n",
    "\n",
    "for subj in SUBJ_CLEAN: \n",
    "    print(f'Compute subj-{subj}')\n",
    "\n",
    "    filename_cond1, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-src', condition=cond1_name)\n",
    "    filename_cond2, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-src', condition=cond2_name)\n",
    " \n",
    "    filename_cond1 = filename_cond1[:-4]\n",
    "    filename_cond2 = filename_cond2[:-4]\n",
    "\n",
    "    save_cond1 = os.path.join(MRI_PATH, f'sub-{subj}', filename_cond1)\n",
    "    save_cond2 = os.path.join(MRI_PATH, f'sub-{subj}', filename_cond2)\n",
    "    \n",
    "    print('--> Read Source Estimate')\n",
    "    stc_1 = mne.read_source_estimate(save_cond1)\n",
    "    stc_2 = mne.read_source_estimate(save_cond2)\n",
    "    \n",
    "    all_subj_stc_cond1.append(stc_1)\n",
    "    all_subj_stc_cond2.append(stc_2)\n",
    "\n",
    "    \n",
    "print('Len condition 1 :', len(all_subj_stc_cond1)) # Length should be n_subj\n",
    "print('Len condition 2 :', len(all_subj_stc_cond2)) # Length should be n_subj\n",
    "\n",
    "# Average subject together per condition\n",
    "print('Average per condition')\n",
    "grand_ave_cond1 = np.mean(all_subj_stc_cond1)\n",
    "grand_ave_cond2 = np.mean(all_subj_stc_cond2)\n",
    "\n",
    "print('Average --> Done')\n",
    "print(grand_ave_cond1)\n",
    "print(grand_ave_cond2)\n",
    "\n",
    "print('Save file')\n",
    "filename_all_subj_cond1, _ = get_bids_file(RESULT_PATH, subj='all', task=task, stage='erp-morph-src', condition=cond1_name)\n",
    "filename_all_subj_cond2, _ = get_bids_file(RESULT_PATH, subj='all', task=task, stage='erp-morph-src', condition=cond2_name)\n",
    "\n",
    "filename_all_subj_cond1 = filename_all_subj_cond1[:-4]\n",
    "filename_all_subj_cond2 = filename_all_subj_cond2[:-4]\n",
    "\n",
    "save_all_cond1 = os.path.join(MRI_PATH, 'sub-all', filename_all_subj_cond1[:-4])\n",
    "save_all_cond2 = os.path.join(MRI_PATH, 'sub-all', filename_all_subj_cond2[:-4])\n",
    "\n",
    "grand_ave_cond1.save(save_all_cond1, ftype='h5', overwrite=True)\n",
    "grand_ave_cond2.save(save_all_cond2, ftype='h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4f3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-all_task-LaughterActive_run-all_cond-LaughReal_erp-morph-src.fif\n",
      "/home/claraelk/scratch/laughter_data/results/mri/sub-all/sub-all_task-LaughterActive_run-all_cond-LaughReal_erp-morph\n"
     ]
    }
   ],
   "source": [
    "task = 'LaughterActive'\n",
    "cond1_name = 'LaughReal'\n",
    "cond2_name = 'LaughPosed'\n",
    "\n",
    "filename_all_subj_cond1, _ = get_bids_file(RESULT_PATH, subj='all', task=task, stage='erp-morph-src', condition=cond1_name)\n",
    "filename_all_subj_cond2, _ = get_bids_file(RESULT_PATH, subj='all', task=task, stage='erp-morph-src', condition=cond2_name)\n",
    "print(filename_all_subj_cond1)\n",
    "\n",
    "\n",
    "filename_all_subj_cond1 = filename_all_subj_cond1[:-4]\n",
    "filename_all_subj_cond2 = filename_all_subj_cond2[:-4]\n",
    "\n",
    "save_all_cond1 = os.path.join(MRI_PATH, 'sub-all', filename_all_subj_cond1[:-4])\n",
    "save_all_cond2 = os.path.join(MRI_PATH, 'sub-all', filename_all_subj_cond2[:-4])\n",
    "\n",
    "print(save_all_cond1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4fcc4",
   "metadata": {},
   "source": [
    "## Import data averaged per subject for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2489e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subj 01\n"
     ]
    }
   ],
   "source": [
    "contrast_data = []\n",
    "group_all_stc_cond1 = []\n",
    "group_all_stc_cond2 =[]\n",
    "\n",
    "task = 'LaughterActive'\n",
    "cond1_name = 'LaughReal'\n",
    "cond2_name = 'LaughPosed'\n",
    "conditions = cond1_name +'-'+ cond2_name\n",
    "\n",
    "for subj in ['01'] :\n",
    "    print('Loading subj', subj)\n",
    "    filename_contrast, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-contrast', condition=conditions)\n",
    "    filename_cond1, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-contrast', condition=cond1_name)\n",
    "    filename_cond2, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-contrast', condition=cond2_name)\n",
    "\n",
    "    save_contrasts = os.path.join(MRI_PATH, 'sub-all', filename_contrast)\n",
    "\n",
    "    save_cond1 = os.path.join(MRI_PATH, 'sub-all', filename_cond1)\n",
    "    save_cond2 = os.path.join(MRI_PATH, 'sub-all', filename_cond2)\n",
    "\n",
    "    with open(save_contrasts, 'rb') as f:\n",
    "        contrast_subj = pickle.load(f)  \n",
    "\n",
    "    with open(save_cond1, 'rb') as f:\n",
    "        group_all_stc_cond1_subj = pickle.load(f)  \n",
    "\n",
    "    with open(save_cond2, 'rb') as f:\n",
    "        group_all_stc_cond2_subj = pickle.load(f) \n",
    "   \n",
    "    contrast_data.append(contrast_subj)\n",
    "    group_all_stc_cond1.append(group_all_stc_cond1_subj[0])\n",
    "    group_all_stc_cond2.append(group_all_stc_cond2_subj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bc3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_path_cond1 = os.path.join(MRI_PATH, f'sub-01', 'morph',\n",
    "                            f'000_MNE_morph-stc.h5')\n",
    "\n",
    "stc_cond1 = mne.read_source_estimate(stc_path_cond1)\n",
    "\n",
    "data_stc_group1 = np.mean(np.array(group_all_stc_cond1), axis=0)\n",
    "test_cond1 = mne.SourceEstimate(data_stc_group1, \n",
    "                                vertices=stc_cond1.vertices, \n",
    "                                tmin=stc_cond1.tmin, \n",
    "                                tstep=stc_cond1.tstep, \n",
    "                                subject='sub-all')\n",
    "\n",
    "data_stc_group2 = np.mean(np.array(group_all_stc_cond2), axis=0)\n",
    "test_cond2 = mne.SourceEstimate(data_stc_group2, \n",
    "                                vertices=stc_cond1.vertices, \n",
    "                                tmin=stc_cond1.tmin, \n",
    "                                tstep=stc_cond1.tstep, \n",
    "                                subject='sub-all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df60ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done]\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "grand_ave = np.mean([test_cond1, test_cond2]) #, evoked3]).data\n",
    "\n",
    "filname_ave1 = f'sub-all_task-LaughterActive_run-all_cond-{cond1_name}_erp-morph-ratio-stc.h5'\n",
    "path_ave1 = os.path.join(MRI_PATH, 'sub-all', filname_ave1)\n",
    "\n",
    "grand_ave_cond1.save(path_ave1, ftype='h5')\n",
    "\n",
    "filname_ave2 = f'sub-all_task-LaughterActive_run-all_cond-{cond2_name}_erp-morph-ratio-stc.h5'\n",
    "path_ave2 = os.path.join(MRI_PATH, 'sub-all', filname_ave2)\n",
    "\n",
    "grand_ave_cond2.save(path_ave2, ftype='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137a80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = (np.array(grand_ave_cond1.data)-np.array(grand_ave_cond2.data))/np.array(grand_ave_cond2.data)\n",
    "ratio_ave = np.mean(ratio, axis=0)\n",
    "print(ratio_ave.shape)\n",
    "\n",
    "#ratio_name, _ = get_bids_file(RESULT_PATH, subj='all', task=task, stage='erp-morph-ratio', condition=conditions)\n",
    "ratio_name = 'sub-all_task-LaughterActive_run-all_cond-LaughReal-LaughPosed_erp-morph-ratio_ave.pkl'\n",
    "ratio_path = os.path.join(MRI_PATH, 'sub-all', ratio_name)\n",
    "\n",
    "with open(ratio_path, 'wb') as f:\n",
    "    pickle.dump(ratio_ave, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f67ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done]\n"
     ]
    }
   ],
   "source": [
    "# Save Average of the ratio in STC\n",
    "stc_path_cond1 = os.path.join(MRI_PATH, f'sub-01', 'morph',\n",
    "                            f'000_MNE_morph-stc.h5')\n",
    "\n",
    "stc_cond1 = mne.read_source_estimate(stc_path_cond1)\n",
    "\n",
    "avg_stc_ratio = mne.SourceEstimate(ratio, \n",
    "                                vertices=stc_cond1.vertices, \n",
    "                                tmin=stc_cond1.tmin, \n",
    "                                tstep=stc_cond1.tstep, \n",
    "                                subject='sub-all')\n",
    "\n",
    "ratio_ave_name ='sub-all_task-LaughterActive_run-all_cond-LaughReal-LaughPosed_erp-morph-ratio-stc_grand-ave.h5'\n",
    "ratio_ave_path = os.path.join(MRI_PATH, 'sub-all', ratio_ave_name)\n",
    "\n",
    "avg_stc_ratio.save(ratio_ave_path, ftype='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4835a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask = np.isnan(contrast)\n",
    "if np.any(nan_mask):\n",
    "    print(\"Warning: NaN values found in the data. Please address and rerun the analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbdcc4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value: -3.0816553290181176e-10\n",
      "Max value: 2.373586805991708e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Min value:\", np.min(contrast))\n",
    "print(\"Max value:\", np.max(contrast))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab274b4e",
   "metadata": {},
   "source": [
    "## Compute adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "885d688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inverse operator decomposition from /home/claraelk/scratch/laughter_data/results/mri/sub-01/src/sub-01-mag-oct6-inv.fif...\n",
      "    Reading inverse operator info...\n",
      "    [done]\n",
      "    Reading inverse operator decomposition...\n",
      "    [done]\n",
      "    270 x 270 full covariance (kind = 1) found.\n",
      "    Noise covariance matrix read.\n",
      "    24585 x 24585 diagonal covariance (kind = 2) found.\n",
      "    Source covariance matrix read.\n",
      "    24585 x 24585 diagonal covariance (kind = 6) found.\n",
      "    Orientation priors read.\n",
      "    24585 x 24585 diagonal covariance (kind = 5) found.\n",
      "    Depth priors read.\n",
      "    Did not find the desired covariance matrix (kind = 3)\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "    Source spaces transformed to the inverse solution coordinate frame\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "surface source space present ...\n",
      "Computing morph matrix...\n",
      "    Left-hemisphere map read.\n",
      "    Right-hemisphere map read.\n",
      "    18 smooth iterations done.\n",
      "    14 smooth iterations done.\n",
      "[done]\n",
      "[done]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# We have to change the shape for the dot() to work properly\\nX = X.reshape(n_vertices_sample, n_times * n_subjects * 2)\\nprint(\"Morphing data.\")\\nX = morph_mat.dot(X)  # morph_mat is a sparse matrix\\nX = X.reshape(n_vertices_fsave, n_times, n_subjects, 2)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the source space we are morphing to\n",
    "src_fname = '/home/claraelk/scratch/laughter_data/results/mri/anat/subjects/fsaverage/bem/fsaverage-5-src.fif'\n",
    "fname_inv = os.path.join(MRI_PATH, 'sub-01', 'src', 'sub-01-mag-oct6-inv.fif')\n",
    "inverse_operator = mne.minimum_norm.read_inverse_operator(fname_inv)\n",
    "\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "fsave_vertices = [s[\"vertno\"] for s in src]\n",
    "morph_mat = mne.compute_source_morph(\n",
    "    src=inverse_operator[\"src\"],\n",
    "    subject_to=\"fsaverage\",\n",
    "    spacing=fsave_vertices,\n",
    "    subjects_dir=subjects_dir,\n",
    ").morph_mat\n",
    "\n",
    "n_vertices_fsave = morph_mat.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = 27\n",
    "\n",
    "\n",
    "# We have to change the shape for the dot() to work properly\n",
    "group_cond1 = np.array(group_all_stc_cond1)\n",
    "group_cond1 = group_cond1.reshape(n_vertices_sample, n_times * n_subjects)\n",
    "print(\"Morphing data.\")\n",
    "group_cond1 = morph_mat.dot(group_cond1)  # morph_mat is a sparse matrix\n",
    "group_cond1 = group_cond1.reshape(n_vertices_fsave, n_times, n_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0134805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_src = 'sub-02-mag-oct6-inv.fif'\n",
    "path_src = os.path.join(MRI_PATH, 'sub-02', 'src', file_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eba0d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "<class 'mne.source_space.SourceSpaces'>\n"
     ]
    }
   ],
   "source": [
    "src_fname = '/home/claraelk/scratch/laughter_data/results/mri/anat/subjects/fsaverage/bem/fsaverage-5-src.fif'\n",
    "\n",
    "src = mne.read_source_spaces(path_src)\n",
    "print(type(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec1a760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing adjacency.\n",
      "-- number of adjacent vertices : 8195\n"
     ]
    }
   ],
   "source": [
    "# Find adjacency\n",
    "print(\"Computing adjacency.\")\n",
    "adjacency = mne.spatial_src_adjacency(src, dist=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f93e2",
   "metadata": {},
   "source": [
    "## Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a92e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 2401, 8195)\n",
      "27\n",
      "Clustering.\n",
      "Using a threshold of 2.055529\n",
      "stat_fun(H1): min=-6.895321 max=6.250080\n",
      "2 disjoint adjacency sets found\n",
      "Running initial clustering …\n",
      "Found 8 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ab3883b52410cb7edece1038abccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/1023 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Now let's actually do the clustering. This can take a long time...\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustering.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m T_obs, clusters, cluster_p_values, H0 \u001b[38;5;241m=\u001b[39m clu \u001b[38;5;241m=\u001b[39m \u001b[43mspatio_temporal_cluster_1samp_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43madjacency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_down_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_disjoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-358>:10\u001b[0m, in \u001b[0;36mspatio_temporal_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, spatial_exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:1285\u001b[0m, in \u001b[0;36mspatio_temporal_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, spatial_exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1284\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpermutation_cluster_1samp_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_permutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_down_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_down_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_disjoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_disjoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-357>:12\u001b[0m, in \u001b[0;36mpermutation_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:1212\u001b[0m, in \u001b[0;36mpermutation_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Non-parametric cluster-level paired t-test.\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \n\u001b[1;32m   1142\u001b[0m \u001b[38;5;124;03mFor details, see :footcite:p:`MarisOostenveld2007,Sassenhagen2019`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03m.. footbibliography::\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m stat_fun, threshold \u001b[38;5;241m=\u001b[39m _check_fun(X, stat_fun, threshold, tail)\n\u001b[0;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_permutation_cluster_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_permutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_down_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_down_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_disjoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_disjoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:986\u001b[0m, in \u001b[0;36m_permutation_cluster_test\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    980\u001b[0m     this_include \u001b[38;5;241m=\u001b[39m step_down_include\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar(\n\u001b[1;32m    983\u001b[0m     iterable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(orders)),\n\u001b[1;32m    984\u001b[0m     mesg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermuting\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    985\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[0;32m--> 986\u001b[0m     H0 \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmy_do_perm_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_include\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mt_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43morders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# include original (true) ordering\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tail \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# up tail\u001b[39;00m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:738\u001b[0m, in \u001b[0;36m_do_1samp_permutations\u001b[0;34m(X, slices, threshold, tail, adjacency, stat_fun, max_step, include, partitions, t_power, orders, sample_shape, buffer_size, progress_bar)\u001b[0m\n\u001b[1;32m    735\u001b[0m     t_obs_surr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m sample_shape\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Find cluster on randomized stats\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m_find_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_obs_surr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mt_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_power\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m perm_clusters_sums \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(perm_clusters_sums) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# get max with sign info\u001b[39;00m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:440\u001b[0m, in \u001b[0;36m_find_clusters\u001b[0;34m(x, threshold, tail, adjacency, max_step, include, partitions, t_power, show_info)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_in \u001b[38;5;129;01min\u001b[39;00m x_ins:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(x_in):\n\u001b[0;32m--> 440\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43m_find_clusters_1dir_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mndimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m         clusters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    444\u001b[0m         sums\u001b[38;5;241m.\u001b[39mappend(out[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:495\u001b[0m, in \u001b[0;36m_find_clusters_1dir_parts\u001b[0;34m(x, x_in, adjacency, max_step, partitions, t_power, ndimage)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(partitions) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    494\u001b[0m     x_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_and(x_in, partitions \u001b[38;5;241m==\u001b[39m p)\n\u001b[0;32m--> 495\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m_find_clusters_1dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mndimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     clusters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    498\u001b[0m     sums\u001b[38;5;241m.\u001b[39mappend(out[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:541\u001b[0m, in \u001b[0;36m_find_clusters_1dir\u001b[0;34m(x, x_in, adjacency, max_step, t_power, ndimage)\u001b[0m\n\u001b[1;32m    539\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m _get_components(x_in, adjacency)\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(adjacency, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# use temporal adjacency\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m \u001b[43m_get_clusters_st\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madjacency must be a sparse matrix or list\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:275\u001b[0m, in \u001b[0;36m_get_clusters_st\u001b[0;34m(x_in, neighbors, max_step)\u001b[0m\n\u001b[1;32m    273\u001b[0m     keepers[row[start]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(col[start:end])\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_clusters_st_1step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeepers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_clusters_st_multistep(keepers, neighbors,\n\u001b[1;32m    278\u001b[0m                                       max_step)\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:177\u001b[0m, in \u001b[0;36m_get_clusters_st_1step\u001b[0;34m(keepers, neighbors)\u001b[0m\n\u001b[1;32m    175\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keepers):\n\u001b[0;32m--> 177\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43m_get_clusters_spatial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ci, cl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(c):\n\u001b[1;32m    179\u001b[0m         check[ii, cl] \u001b[38;5;241m=\u001b[39m ci \u001b[38;5;241m+\u001b[39m enum_offset\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:139\u001b[0m, in \u001b[0;36m_get_clusters_spatial\u001b[0;34m(s, neighbors)\u001b[0m\n\u001b[1;32m    137\u001b[0m ind \u001b[38;5;241m=\u001b[39m t_inds[icount \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# look across other vertices\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m buddies \u001b[38;5;241m=\u001b[39m \u001b[43m_get_buddies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m t_inds\u001b[38;5;241m.\u001b[39mextend(buddies)\n\u001b[1;32m    141\u001b[0m icount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:30\u001b[0m, in \u001b[0;36m_get_buddies_fallback\u001b[0;34m(r, s, neighbors, indices)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     buddies \u001b[38;5;241m=\u001b[39m indices[r[indices]]\n\u001b[0;32m---> 30\u001b[0m buddies \u001b[38;5;241m=\u001b[39m buddies[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuddies\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_unique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[1;32m     31\u001b[0m r[buddies] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buddies\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Note that X needs to be a multi-dimensional array of shape\n",
    "# observations (subjects) × time × space, so we permute dimensions\n",
    "X = np.transpose(contrast, [0, 2, 1])\n",
    "print(X.shape)\n",
    "n_subjects = len(contrast)\n",
    "print(n_subjects)\n",
    "\n",
    "# Here we set a cluster forming threshold based on a p-value for\n",
    "# the cluster based permutation test.\n",
    "# We use a two-tailed threshold, the \"1 - p_threshold\" is needed\n",
    "# because for two-tailed tests we must specify a positive threshold.\n",
    "p_threshold = 0.001\n",
    "df = n_subjects - 1  # degrees of freedom for the test\n",
    "t_threshold = stats.distributions.t.ppf(1 - p_threshold / 2, df=df)\n",
    "\n",
    "# Now let's actually do the clustering. This can take a long time...\n",
    "print(\"Clustering.\")\n",
    "T_obs, clusters, cluster_p_values, H0 = clu = spatio_temporal_cluster_1samp_test(\n",
    "    X,\n",
    "    adjacency=adjacency,\n",
    "    n_jobs=-1,\n",
    "    threshold=None,\n",
    "    buffer_size=None,\n",
    "    verbose=True,\n",
    "    n_permutations=1024,\n",
    "    tail=0,\n",
    "    step_down_p = 0.05,\n",
    "    check_disjoint=True,\n",
    "    out_type='indices'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53c73c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cluster\n"
     ]
    }
   ],
   "source": [
    "print('Loading cluster')\n",
    "task = 'LaughterActive'\n",
    "cond1_name = 'LaughReal'\n",
    "cond2_name = 'LaughPosed'\n",
    "\n",
    "conditions = f'{cond1_name}-{cond2_name}'\n",
    "\n",
    "save_cluster_stats, _ = get_bids_file(RESULT_PATH, stage = \"erp-source\", task=task, measure=\"Ttest-clusters-src\", condition = conditions)\n",
    "\n",
    "path_save_cluster = os.path.join(MRI_PATH, 'sub-all', save_cluster_stats)\n",
    "\n",
    "with open(path_save_cluster, 'rb') as f:\n",
    "    clu = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b60119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/claraelk/scratch/laughter_data/results/mri/sub-all/sub-all_task-LaughterActive_run-all_cond-LaughReal-LaughPosed_meas-Ttest-clusters-src_erp-source.fif'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_save_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a62377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "T_obs, clusters, cluster_p_values, H0 = clu \n",
    "\n",
    "# Select the clusters that are statistically significant at p < 0.05\n",
    "good_clusters_idx = np.where(cluster_p_values < 0.05)[0]\n",
    "good_clusters = [clusters[idx] for idx in good_clusters_idx]\n",
    "print(good_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7049f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_p_values == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca78c520",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1496c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape contrast : (27, 20484, 1801)\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "<class 'mne.source_space.SourceSpaces'>\n",
      "Computing adjacency.\n",
      "-- number of adjacent vertices : 20484\n",
      "(20484, 20484)\n",
      "(27, 1801, 20484)\n",
      "27\n",
      "Clustering.\n",
      "Using a threshold of 2.055529\n",
      "stat_fun(H1): min=-6.911755 max=6.263853\n",
      "Running initial clustering …\n",
      "Found 28091 clusters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Now let's actually do the clustering. This can take a long time...\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustering.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m T_obs, clusters, cluster_p_values, H0 \u001b[38;5;241m=\u001b[39m clu \u001b[38;5;241m=\u001b[39m \u001b[43mspatio_temporal_cluster_1samp_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43madjacency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_perm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_disjoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     79\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Select the clusters that are statistically significant at p < 0.05\u001b[39;00m\n\u001b[1;32m     82\u001b[0m good_clusters_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(cluster_p_values \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.05\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m<decorator-gen-358>:10\u001b[0m, in \u001b[0;36mspatio_temporal_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, spatial_exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:1285\u001b[0m, in \u001b[0;36mspatio_temporal_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, spatial_exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1284\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpermutation_cluster_1samp_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_permutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_down_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_down_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_disjoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_disjoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-357>:12\u001b[0m, in \u001b[0;36mpermutation_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:1212\u001b[0m, in \u001b[0;36mpermutation_cluster_1samp_test\u001b[0;34m(X, threshold, n_permutations, tail, stat_fun, adjacency, n_jobs, seed, max_step, exclude, step_down_p, t_power, out_type, check_disjoint, buffer_size, verbose)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Non-parametric cluster-level paired t-test.\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \n\u001b[1;32m   1142\u001b[0m \u001b[38;5;124;03mFor details, see :footcite:p:`MarisOostenveld2007,Sassenhagen2019`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03m.. footbibliography::\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m stat_fun, threshold \u001b[38;5;241m=\u001b[39m _check_fun(X, stat_fun, threshold, tail)\n\u001b[0;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_permutation_cluster_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_permutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_down_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_down_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_disjoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_disjoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:944\u001b[0m, in \u001b[0;36m_permutation_cluster_test\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    942\u001b[0m     X_full \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    943\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m     orders, n_permutations, extra \u001b[38;5;241m=\u001b[39m \u001b[43m_get_1samp_orders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     n_permutations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_permutations)\n",
      "File \u001b[0;32m~/notebook/lib/python3.8/site-packages/mne/stats/cluster_level.py:819\u001b[0m, in \u001b[0;36m_get_1samp_orders\u001b[0;34m(n_samples, n_permutations, tail, rng)\u001b[0m\n\u001b[1;32m    817\u001b[0m use_samples \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;241m-\u001b[39m (tail \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ii \u001b[38;5;241m<\u001b[39m n_permutations \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 819\u001b[0m     signs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((\u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m signs \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hashes:\n\u001b[1;32m    821\u001b[0m         orders[ii, :use_samples] \u001b[38;5;241m=\u001b[39m signs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task = 'LaughterActive'\n",
    "cond1_name = 'LaughReal'\n",
    "cond2_name = 'LaughPosed'\n",
    "\n",
    "contrast_data = []\n",
    "group_all_stc_cond1 = []\n",
    "group_all_stc_cond2 =[]\n",
    "conditions = cond1_name + \"-\" + cond2_name\n",
    "\n",
    "crop = True\n",
    "tmin = 0.0\n",
    "tmax = 1.5\n",
    "\n",
    "##### Import data\n",
    "for subj in SUBJ_CLEAN :\n",
    "    filename_cond1, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-src', condition=cond1_name)\n",
    "    filename_cond2, _ = get_bids_file(RESULT_PATH, subj=subj, task=task, stage='erp-morph-src', condition=cond2_name)\n",
    "\n",
    "    save_cond1 = os.path.join(MRI_PATH, f'sub-{subj}', filename_cond1[:-4])\n",
    "    save_cond2 = os.path.join(MRI_PATH, f'sub-{subj}', filename_cond2[:-4])\n",
    "\n",
    "    stc_cond1 = mne.read_source_estimate(save_cond1)\n",
    "    stc_cond2 = mne.read_source_estimate(save_cond2)\n",
    "\n",
    "    if crop == True : \n",
    "        stc_cond1.crop(tmin, tmax)\n",
    "        stc_cond2.crop(tmin, tmax)\n",
    "\n",
    "    group_all_stc_cond1.append(stc_cond1.data)\n",
    "    group_all_stc_cond2.append(stc_cond2.data)\n",
    "\n",
    "###### Compute contrast\n",
    "contrast = np.array(group_all_stc_cond1)-np.array(group_all_stc_cond2) \n",
    "print('Shape contrast :', contrast.shape)\n",
    "\n",
    "###### Compute adjacency #######\n",
    "src_fname = os.path.join(MRI_PATH, 'anat/subjects', 'fsaverage/bem', 'fsaverage-5-src.fif')\n",
    "\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "print(type(src))\n",
    "\n",
    "# Find adjacency\n",
    "print(\"Computing adjacency.\")\n",
    "adjacency = mne.spatial_src_adjacency(src, dist=None)\n",
    "print(adjacency.shape)\n",
    "\n",
    "# Note that X needs to be a multi-dimensional array of shape\n",
    "# observations (subjects) × time × space, so we permute dimensions\n",
    "X = np.transpose(contrast, [0, 2, 1])\n",
    "print(X.shape)\n",
    "n_subjects = len(contrast)\n",
    "print(n_subjects)\n",
    "\n",
    "\n",
    "########### STATISTICAL ANALYSIS ##########\n",
    "\n",
    "# Here we set a cluster forming threshold based on a p-value for\n",
    "# the cluster based permutation test.\n",
    "# We use a two-tailed threshold, the \"1 - p_threshold\" is needed\n",
    "# because for two-tailed tests we must specify a positive threshold.\n",
    "p_threshold = 0.001\n",
    "df = n_subjects - 1  # degrees of freedom for the test\n",
    "t_threshold = stats.distributions.t.ppf(1 - p_threshold / 2, df=df)\n",
    "n_perm = 'all'\n",
    "\n",
    "# Now let's actually do the clustering. This can take a long time...\n",
    "print(\"Clustering.\")\n",
    "T_obs, clusters, cluster_p_values, H0 = clu = spatio_temporal_cluster_1samp_test(\n",
    "    X,\n",
    "    adjacency=adjacency,\n",
    "    n_jobs=-1,\n",
    "    threshold=None,\n",
    "    buffer_size=None,\n",
    "    verbose=True,\n",
    "    n_permutations= n_perm,\n",
    "    tail=0,\n",
    "    check_disjoint=False,\n",
    "    out_type='indices'\n",
    ")\n",
    "\n",
    "# Select the clusters that are statistically significant at p < 0.05\n",
    "good_clusters_idx = np.where(cluster_p_values < 0.05)[0]\n",
    "good_clusters = [clusters[idx] for idx in good_clusters_idx]\n",
    "print(good_clusters)\n",
    "\n",
    "print('Saving cluster')\n",
    "if crop == False : \n",
    "    save_cluster_stats, _ = get_bids_file(RESULT_PATH, stage = \"erp-src\", task=task, \n",
    "                                          measure=f\"Ttest-clusters-morph-perm-{n_perm}\", \n",
    "                                          condition = conditions)\n",
    "\n",
    "else :\n",
    "    save_cluster_stats, _ = get_bids_file(RESULT_PATH, stage = \"erp-src\", task=task, \n",
    "                                          measure=f\"Ttest-clusters-morph-perm-{n_perm}_time{tmin}-{tmax}\", \n",
    "                                          condition = conditions)\n",
    "\n",
    "path_save_cluster = os.path.join(MRI_PATH, 'sub-all', save_cluster_stats)\n",
    "\n",
    "#with open(path_save_cluster, 'wb') as f:\n",
    "#    pickle.dump(clu, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477d099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
